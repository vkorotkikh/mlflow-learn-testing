version: '3.8'

services:
  # MLflow Tracking Server
  mlflow:
    build:
      context: ..
      dockerfile: docker/Dockerfile.mlflow
    container_name: mlflow-server
    ports:
      - "5000:5000"
    volumes:
      - mlflow_artifacts:/opt/mlflow/artifacts
      - mlflow_db:/opt/mlflow/db
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///db/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/opt/mlflow/artifacts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - mlops-network

  # PostgreSQL for MLflow (optional - for production)
  postgres:
    image: postgres:13
    container_name: mlflow-postgres
    environment:
      POSTGRES_DB: mlflow
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow123
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - mlops-network

  # MLflow with PostgreSQL backend
  mlflow-postgres:
    build:
      context: ..
      dockerfile: docker/Dockerfile.mlflow
    container_name: mlflow-server-postgres
    ports:
      - "5001:5000"
    volumes:
      - mlflow_artifacts:/opt/mlflow/artifacts
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow123@postgres:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/opt/mlflow/artifacts
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      sh -c "mlflow server 
             --host 0.0.0.0 
             --port 5000 
             --backend-store-uri postgresql://mlflow:mlflow123@postgres:5432/mlflow 
             --default-artifact-root /opt/mlflow/artifacts 
             --serve-artifacts"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - mlops-network
    profiles:
      - postgres

  # Pipeline Executor
  pipeline-executor:
    build:
      context: ..
      dockerfile: docker/Dockerfile.pipeline
    container_name: pipeline-executor
    volumes:
      - ../models:/app/models
      - ../config:/app/config
      - pipeline_outputs:/app/outputs
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - PYTHONPATH=/app
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - mlops-network
    profiles:
      - pipeline

  # Jupyter Notebook for experimentation
  jupyter:
    build:
      context: ..
      dockerfile: docker/Dockerfile.pipeline
    container_name: mlops-jupyter
    ports:
      - "8888:8888"
    volumes:
      - ..:/app
      - jupyter_data:/home/appuser/.jupyter
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - JUPYTER_ENABLE_LAB=yes
    command: >
      sh -c "pip install jupyterlab && 
             jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root 
             --NotebookApp.token='' --NotebookApp.password=''"
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - mlops-network
    profiles:
      - jupyter

  # Airflow (Simplified setup)
  airflow:
    image: apache/airflow:2.7.1-python3.9
    container_name: airflow-standalone
    ports:
      - "8080:8080"
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/plugins:/opt/airflow/plugins
      - airflow_logs:/opt/airflow/logs
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - KUBEFLOW_ENDPOINT=http://localhost:8080
    command: >
      sh -c "airflow db init &&
             airflow users create --username admin --firstname Admin --lastname User 
             --role Admin --email admin@example.com --password admin &&
             airflow webserver --port 8080 & airflow scheduler"
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - mlops-network
    profiles:
      - airflow

volumes:
  mlflow_artifacts:
    driver: local
  mlflow_db:
    driver: local
  postgres_data:
    driver: local
  pipeline_outputs:
    driver: local
  jupyter_data:
    driver: local
  airflow_logs:
    driver: local

networks:
  mlops-network:
    driver: bridge


